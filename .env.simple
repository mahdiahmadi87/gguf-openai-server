# .env file

# --- REQUIRED ---
ALLOWED_API_KEYS="sk-local-key-dev,sk-another-key"

# --- REQUIRED: Configure your models here as a JSON string ---
MODELS_CONFIG='[ \
  {"model_id": "gemma-3-4b", "model_path": "/home/mahdi/Documents/LLM/gguf-openai-server/models/gemma-3-4b-it-q4_0.gguf", "n_gpu_layers": -1, "n_ctx": 4096, "is_multimodal": true}, \
  {"model_id": "mistral-7b-text", "model_path": "/path/to/your/models/mistral-7b-v0.1.Q4_K_M.gguf", "n_gpu_layers": -1, "n_ctx": 4096, "is_multimodal": false} \
]'



# Note: Using backslashes (\) for multi-line readability in the .env example,
# but it can also be a single compact line. JSON requires double quotes.
# Optional fields like n_ctx, n_gpu_layers, mmproj_path, chat_format will use defaults if omitted.

              
# --- OPTIONAL ---
HOST="0.0.0.0"
PORT=8000
LOG_LEVEL="INFO"
VERBOSE_LLAMA=True  
# DEFAULT_N_GPU_LAYERS=-1 # Defaults can still be set
# DEFAULT_N_CTX=4096