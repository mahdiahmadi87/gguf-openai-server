# .env - Example Configuration File

# --- Required ---
# Full path to your downloaded GGUF model file
# Make sure this path is correct for your system
MODEL_PATH="/home/user/models/mistral-7b-instruct-v0.2.Q5_K_M.gguf"

# --- API Security ---
# Comma-separated list of valid API keys clients must provide
# Generate your own secure keys. If this line is commented out or empty,
# the API will be open to anyone (NO AUTHENTICATION).
API_KEYS="sk-local-abcdef123456,sk-local-xyz7890abc"

# --- Model Configuration ---
# This is the identifier clients will use in their API requests (e.g., in the 'model' field)
MODEL_NAME="mistral-instruct-local"

# --- llama-cpp-python Parameters ---
# Number of GPU layers to offload.
# -1 means try to offload all layers (requires sufficient VRAM).
# 0 means use CPU only.
# Adjust based on your GPU's VRAM. Check llama-cpp-python docs for details.
N_GPU_LAYERS=20 # Example: Offload 20 layers to GPU

# Model context window size (max sequence length)
# Check the model's documentation for its maximum supported context size.
N_CTX=4096

# Batch size for prompt processing (higher can improve throughput but uses more VRAM)
N_BATCH=512

# Set to true for verbose logging from the llama.cpp backend itself
LLAMA_VERBOSE=False

# --- Server Configuration ---
# IP address to bind the server to. 0.0.0.0 makes it accessible on your network.
# Use 127.0.0.1 to only allow connections from the same machine.
HOST="0.0.0.0"

# Port the server will listen on
PORT=8000

# Logging level for the FastAPI application (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL="INFO"